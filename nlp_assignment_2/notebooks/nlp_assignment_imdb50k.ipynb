{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94227ab3",
   "metadata": {},
   "source": [
    "\n",
    "# 📝 Assignment: Intro NLP Sentiment Analysis (IMDB 50K)\n",
    "\n",
    "**Total points: 100**  \n",
    "**Estimated time:** 90–120 minutes  \n",
    "**Prereqs:** You completed the intro notebook (cleaning → BoW/TF‑IDF → simple classifier).\n",
    "\n",
    "This assignment uses the **IMDB Dataset of 50K Movie Reviews** (binary sentiment).  \n",
    "On Kaggle, add the dataset **“IMDB Dataset of 50K Movie Reviews”** and use the mounted path:\n",
    "```\n",
    "/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\n",
    "```\n",
    "If you're running locally, download the CSV and set the path accordingly. Provided in Classroom. \n",
    "\n",
    "> You will implement a complete classical NLP pipeline: **load → clean → vectorize → model → evaluate → analyze**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e69db02",
   "metadata": {},
   "source": [
    "\n",
    "## 🎯 Learning outcomes\n",
    "By the end, you can:\n",
    "- Load a real‑world text dataset and standardize labels.\n",
    "- Implement minimal text cleaning (lowercase, punctuation, whitespace).\n",
    "- Build **Bag‑of‑Words** and **TF‑IDF** features with scikit‑learn.\n",
    "- Train/evaluate **Multinomial Naive Bayes** and **Logistic Regression**.\n",
    "- Compare models and **inspect influential features**.\n",
    "- Perform brief **error analysis** to inform next steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c395d6b3",
   "metadata": {},
   "source": [
    "\n",
    "## ✅ Rules & tips\n",
    "- Keep runtime modest: you may **subsample to N_SAMPLES = 8000** for faster iteration.\n",
    "- Use **matplotlib** only (no seaborn), **one plot per cell**, and avoid specifying custom colors.\n",
    "- Set `RANDOM_SEED = 42` for reproducibility.\n",
    "- Write short Markdown answers where requested.\n",
    "- **Do not** install heavy external libraries (no NLTK download required).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ad343c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, re, string, math, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682edd9e",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Part 0 — Load the IMDB dataset (10 pts)\n",
    "\n",
    "**Task:** Implement `load_imdb_csv(path)` so it returns a DataFrame with columns:\n",
    "- `text` (review text, `str`)\n",
    "- `label` (`int`: 1 for positive, 0 for negative)\n",
    "\n",
    "**Notes**\n",
    "- The Kaggle CSV has columns **`review`** and **`sentiment`** (`\"positive\"`/`\"negative\"`).\n",
    "- Map to integers (`positive→1`, `negative→0`), drop rows with missing values, reset index.\n",
    "- If `N_SAMPLES` is not `None`, randomly sample that many rows **stratified by label**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b49e276",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CSV_PATH = \"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\"\n",
    "# Or use the following if you downloaded it:\n",
    "# CSV_PATH = \"IMDB Dataset.csv\"\n",
    "\n",
    "N_SAMPLES = 8000  # set to None for full 50K\n",
    "\n",
    "def load_imdb_csv(path: str, n_samples=None, random_state=42) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return DataFrame with columns:\n",
    "      - text: str\n",
    "      - label: int (1 for positive, 0 for negative)\n",
    "    Apply optional stratified sampling if n_samples is provided.\n",
    "    \"\"\"\n",
    "    # TODO (10 pts):\n",
    "    # 1) Read CSV\n",
    "    # 2) Rename columns => text, label\n",
    "    # 3) Map label strings to integers\n",
    "    # 4) Drop NaNs and reset index\n",
    "    # 5) If n_samples is not None, do a stratified sample by label\n",
    "    raise NotImplementedError(\"Implement load_imdb_csv()\")\n",
    "\n",
    "# df = load_imdb_csv(CSV_PATH, n_samples=N_SAMPLES)\n",
    "# df.head()\n",
    "# df['label'].value_counts(normalize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c033b0af",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Part 1 — Minimal text cleaning (10 pts)\n",
    "\n",
    "Implement `clean_text(s)` to:\n",
    "1. Lowercase\n",
    "2. Remove punctuation\n",
    "3. Collapse multiple spaces to a single space; strip\n",
    "\n",
    "Then create:\n",
    "```python\n",
    "df['clean'] = df['text'].apply(clean_text)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df90ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    # TODO (10 pts):\n",
    "    # text = s.lower()\n",
    "    # table = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    # text = text.translate(table)\n",
    "    # text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    # return text\n",
    "    raise NotImplementedError(\"Implement clean_text()\")\n",
    "\n",
    "# df['clean'] = df['text'].apply(clean_text)\n",
    "# df[['text', 'clean']].head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92c8218",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Part 2 — Train/test split (5 pts)\n",
    "\n",
    "Split the data into train/test:\n",
    "```python\n",
    "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
    "    df['clean'], df['label'], test_size=0.2, random_state=RANDOM_SEED, stratify=df['label']\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef438f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO (5 pts): create X_train_text, X_test_text, y_train, y_test as described\n",
    "raise NotImplementedError(\"Create train/test split\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faafe1a",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Part 3 — Bag‑of‑Words with `CountVectorizer` (10 pts)\n",
    "\n",
    "Vectorize the texts using:\n",
    "```python\n",
    "cv = CountVectorizer(stop_words='english', min_df=2)\n",
    "Xtr_bow = cv.fit_transform(X_train_text)\n",
    "Xte_bow = cv.transform(X_test_text)\n",
    "```\n",
    "**Deliverables:** variables `cv`, `Xtr_bow`, `Xte_bow`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9fe20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO (10 pts): create BoW features\n",
    "raise NotImplementedError(\"Create CountVectorizer features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3547ad7",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Part 4 — Train **Multinomial Naive Bayes** & evaluate (15 pts)\n",
    "\n",
    "Train and evaluate:\n",
    "```python\n",
    "nb = MultinomialNB()\n",
    "nb.fit(Xtr_bow, y_train)\n",
    "y_pred_nb = nb.predict(Xte_bow)\n",
    "```\n",
    "**Report:** accuracy, precision, recall, F1 (binary average). Then plot a **confusion matrix**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef630321",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def report_metrics(y_true, y_pred, title=None):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
    "    print(f\"Accuracy={acc:.3f}  Precision={p:.3f}  Recall={r:.3f}  F1={f1:.3f}\")\n",
    "    print()\n",
    "    print(classification_report(y_true, y_pred, digits=3))\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=[0,1])\n",
    "    disp.plot()\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# TODO (15 pts): train NB on BoW, predict, and call report_metrics\n",
    "raise NotImplementedError(\"Train/evaluate MultinomialNB on BoW\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b0d758",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Part 5 — TF‑IDF features + Naive Bayes (10 pts)\n",
    "\n",
    "Build TF‑IDF features and repeat NB:\n",
    "```python\n",
    "tfidf = TfidfVectorizer(stop_words='english', min_df=2)\n",
    "Xtr_tfidf = tfidf.fit_transform(X_train_text)\n",
    "Xte_tfidf = tfidf.transform(X_test_text)\n",
    "```\n",
    "Compare metrics vs. BoW (**1–3 sentences** in a Markdown cell).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf45738",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO (10 pts): TF-IDF features + NB training/eval; store predictions in y_pred_nb_tfidf\n",
    "raise NotImplementedError(\"TF-IDF + NB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fccc6b",
   "metadata": {},
   "source": [
    "\n",
    "👉 **Answer (Markdown, 5 pts):** In **1–3 sentences**, compare BoW vs TF‑IDF performance and briefly speculate why.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388f4aa1",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Part 6 — Add bigrams & try Logistic Regression (20 pts)\n",
    "\n",
    "Create bigram TF‑IDF features and train `LogisticRegression(max_iter=1000)`:\n",
    "```python\n",
    "tfidf_bg = TfidfVectorizer(ngram_range=(1,2), stop_words='english', min_df=2)\n",
    "Xtr_bg = tfidf_bg.fit_transform(X_train_text)\n",
    "Xte_bg = tfidf_bg.transform(X_test_text)\n",
    "\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "logreg.fit(Xtr_bg, y_train)\n",
    "y_pred_lr = logreg.predict(Xte_bg)\n",
    "```\n",
    "Report metrics and **compare** to NB.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eca240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO (20 pts): bigram TF-IDF + Logistic Regression training/eval\n",
    "raise NotImplementedError(\"Bigrams + Logistic Regression\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc94c11",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Part 7 — Inspect top indicative features (10 pts)\n",
    "\n",
    "Using the **Logistic Regression** coefficients on the bigram TF‑IDF features:\n",
    "- Get feature names: `tfidf_bg.get_feature_names_out()`\n",
    "- For class 1 (positive), list the top 20 features with largest coefficients.\n",
    "- For class 0 (negative), list the top 20 with smallest coefficients.\n",
    "\n",
    "Print them as simple lists or a small DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b50450e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO (10 pts): print top positive and top negative features by coefficient magnitude\n",
    "raise NotImplementedError(\"Top features from Logistic Regression coefficients\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7393c871",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Part 8 — Error analysis (10 pts)\n",
    "\n",
    "Print **5 false positives** and **5 false negatives** based on your **best** model.  \n",
    "For each, print the **true label**, **predicted label**, and the **original review text** (truncated to ~200 chars).\n",
    "\n",
    "> Optional: If you used Logistic Regression, you may also inspect `decision_function` to sort by confidence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40437604",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO (10 pts): print examples of FP and FN\n",
    "raise NotImplementedError(\"Error analysis examples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22c3ecb",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## ✍️ Short reflection (5 pts)\n",
    "\n",
    "In **5–8 sentences**, summarize:\n",
    "- Which combination (features + model) worked best?\n",
    "- One thing you learned from top‑feature inspection.\n",
    "- One idea you’d try next (e.g., character n‑grams, regularization `C`, class weights, normalization).\n",
    "\n",
    "*(Write your answer in the cell below.)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2221f043",
   "metadata": {},
   "source": [
    "\n",
    "**Your reflection here…**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a82b2e5",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## (Optional) Save predictions\n",
    "\n",
    "If you want to keep a record of your predictions for the best model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c513d61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example stub (uncomment and adapt):\n",
    "# out = pd.DataFrame({\n",
    "#     \"text\": X_test_text.reset_index(drop=True),\n",
    "#     \"label_true\": y_test.reset_index(drop=True),\n",
    "#     \"label_pred\": y_pred_lr  # or your best model's predictions\n",
    "# })\n",
    "# out.to_csv(\"imdb_predictions.csv\", index=False)\n",
    "# print(\"Saved to imdb_predictions.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0930e2c5",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## ✅ Checklist before you submit\n",
    "- [ ] All TODOs implemented (no `NotImplementedError` left).\n",
    "- [ ] At least **one confusion matrix** plotted.\n",
    "- [ ] Part 5 Markdown comparison written (1–3 sentences).\n",
    "- [ ] Reflection written (5–8 sentences).\n",
    "- [ ] Notebook runs top‑to‑bottom on a fresh kernel.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
